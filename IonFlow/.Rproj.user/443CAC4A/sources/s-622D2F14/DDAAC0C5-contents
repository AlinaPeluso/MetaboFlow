---
title: 'IonFlow: Pipeline for processing and analysis of Ionomics data'
author: "Alina Peluso <alina.peluso@gmail.com>"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: BibliographyR.bib
geometry: "left=1cm,right=1cm,top=2cm,bottom=2cm"
output:
  pdf_document: default
---

<style type="text/css">
code.r{
  font-size: 8px;
}
</style>


```{r global_options, include=FALSE, warning=FALSE}
options(width=110)
options("scipen"=100, "digits"=2)
knitr::opts_chunk$set(warning=FALSE)
```



This package illustrate tools for processing metabolomics data to aid reproducible data sharing and big data initiatives. 

The workflow consist of five sections, and respectively

* Pre-processing,
* Exploratory analysis,
* Clustering which also includes the GO Slim annotation and GO terms enrichment,
* Network analysis.


The workflow is wrapped within the `r IonFlow` R package.

```{r, echo=FALSE}
# Import the package
install.packages("devtools") # if you have not installed "devtools" package
devtools::install_github("AlinaPeluso/IonFlow")

library(IonFlow)
```



# Pre-processing

The pre-processing section is required first as it produces in output the cleaned dataset to be used in the other sections. There is no specific order on how to run the other sections. 

The pre-processing section aims to free the data from unreliable samples which will probably lead to wrong outputs. In such way, effective data pre-processing methods are applied to avoid the effects of noisy and unreliable data.

This section requires as input the raw data frame, e.g. ion's concentrations. It is also possible to define a set of ion's standard deviation, as these are possibly computed accounting for some control genes. Note that the latter is an optional input i.e if not provided the standard deviations from the data would be computed to perform the data standardisation (see Section \@ref(Standardisation)).

#### Inspect the raw data

```{r, echo=FALSE}
data(IonData)
```

We illustrate the Ionomics workflow with ICP-MS data of yeast intracellular ion concentrations measured for 1454 single-gene haploid knockouts [see @danku2009high]. Ions measured include Ca44, Cd111, Co59, Cu65, Fe56, K39, Mg25, Mn55, Mo95, Na23, Ni60, P31, S34, Zn66. Values of concentration are in ppm and have been adjusted using optical density measurements. Intracellular concentrations are measured for two, four or eight replicas of each mutant depending on the knock-out. Knock-out YDL227C mutant is measured multiple times in every batch as control strain.


#### Run the pre-processing function

```{r, echo=FALSE, fig.show="hide", results="hide", cache=TRUE}
pre_proc <- PreProcessing(data=IonData,stdev=pre_defined_sd)
```

The concentration values for the raw data ion can be summarised as follow.

```{r, echo=FALSE}
pre_proc$stats.raw_data
```

There is a very high variability of the knockouts across ions and within the batches.
There are no missing values in the data.


#### Outlier detection

We define a lower outer fence: $Q1 - 3*IQ$ and a upper outer fence: $Q3 + 3*IQ$ where $Q1$ and $Q3$ are the first and the third quantile of the distribution, respectively. A point beyond the outer fence is considered an extreme outlier.

The outliers are split across ions as follows. 

```{r, echo=FALSE}
pre_proc$stats.outliers
```


#### Median batch correction

First we take the logarithmm of the concentration value. Then, the data are scaled to the median taken for each ion within each batch.

```{r, echo=FALSE}
pre_proc$stats.median_batch_corrected_data
```

After outlier removal and the median batch correction of the logged concentrations (logConcentration_corr), the data looks as

```{r, echo=FALSE}
pre_proc$plot.logConcentration_by_batch 
```

#### Standardisation {#Standardisation}

After outlier removal and median batch correction we now standardise the ions' logged concentrations. For each set of knockouts across the ions's type, we normalise the concentrations by dividing for the ions' standard deviation. The ions' standard deviations can be estimated from the data, or a set of pre-defined ions' standard deviations can be used. The latter has been computed on the complete dataset (which includes also some gene controls). At the moment we do not use the pre-defined ion's concenrations to normalise our data.


The concentration values for each ion can be summarised as follow.

```{r, echo=FALSE}
pre_proc$stats.standardised_data
```



#### Symbolization
As we are working with the logConcentration_corr_norm we can consider a thresold based on a certain number of sigma (e.g. number-of-sigma thresold=3) to symbolizise the concentrations' profile of the knockouts as follow:

* Symb=0  if -3<logConcentration_corr_norm<3 
* Symb=1  if logConcentration_corr_norm>=3 
* Symb=-1 otherwise

The choice of the thresold is arbitrary i.e. thresold greater than 3 can be chosen. The highest is the thresold, the highest is the concentration value taken as significant.


#### Aggregation of the knockout replicates

For each ion measure, we proceed to aggregate the data by taking the median value of the knockout. For each ions we consider around 1,450 genes. And of which we can plot the z-score with the associated sigma as follow.

```{r, fig.align="center", echo=FALSE}
pre_proc$plot.logConcentration_z_scores
```


#### Final datasets

Three dataset are obtained as output. The first in the long format (genes as rows and ions as columns), and two in wide format and respectively one with the standardised ion's concentraction, and the other with the symbolised profiles of the knockouts.

Long format (aggregated knockout replicates):
```{r}
head(pre_proc$dataR.long)
```

Long format (not aggregated knockout replicates):
```{r}
head(pre_proc$data.long)
```

Wide format, standardised ion's concentraction:
```{r}
head(pre_proc$data.wide)
```

Wide format, symbolised profiles:
```{r}
head(pre_proc$data.wide_Symb)
```



# Exploratory analysis


This section provide a way to summarize the main characteristics of the data with visual methods.
No input need to be supplied as this section is built on the output of the previous section.


```{r, echo=FALSE, cache=TRUE, fig.show="hide", results="hide"}
exp_anal <- ExploratoryAnalysis(data=pre_proc$data.wide)
```


#### Pearson correlation


```{r, echo=FALSE, fig.align='center'}
exp_anal$plot.Pearson_correlation 
```


#### PCA


The aim of PCA is to reduce the dimensionality of the data while retaining as much information as possible. This is achieved by projecting the data into a new lower-dimensional space defined by the principal components (PC) that combine in a linear way the original (possibly correlated) variables (e.g. ions) in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, it means that each gene is assigned a score on each new PC dimension, and this score is calculated by appling weight to a a linear combination of the original variables.
In our case, the data are centred but not further scaled as were normalised in the pre-processing stage thus the variance is homogeneous across variables. The algorithm is able to handle missing values.

```{r, echo=FALSE, fig.align='center'}
exp_anal$plot.PCA_Individual
```

The weights of each of the original variables are stored in the so-called loading vectors associated to each PC.

Loadings (first 10) for PC1:

```{r, echo=FALSE}
head(exp_anal$stat.loadings_PC1,10)
```

Loadings (first 10) for PC2:

```{r, echo=FALSE}
head(exp_anal$stat.loadings_PC2,10)
```

#### Heatmap

We employ an heatmap as a graphical representation of data where the knockout values contained are represented as colors. A dendrogram is also added to the left side (clustering of genes knockout) and to the top (clustering of ions).

```{r, echo=FALSE, out.width = "50%", fig.height = 6, fig.align='center'}
exp_anal$plot.heatmap 
```



#### Pairwise correlation map

We employ a correlation map to visualise the paiwise correlation coefficients across ions.

```{r, echo=FALSE, fig.align='center'}
exp_anal$plot.pairwise_correlation_map 
```

#### Regularized partial correlation network 

We now inspect the (statistical) relationships between ions in the form of graphical models. The graph is made of n nodes (ions) connected by m edges (knockout) and the relationships between ions is visualised as weighted edges. 

We compute a network of partial correlation coefficients. Such networks can also be termed concentration graphs (Cox & Wermuth, 1994) or Gaussian graphical models (Lauritzen, 1996). Each link in the network represents a partial correlation coefficient between two variables after conditioning on all other variables in the dataset. These coefficients range from $-1$ to $1$ and encode the remaining association between two nodes after controlling for all other information possible, also known as conditional independence associations. 

The connections are visualized using red lines indicating negative partial correlations, green lines indicating positive partial correlations, and wider and more saturated connections indicate partial correlations that are far from zero. 

Whenever the partial correlation is exactly zero, no connection is drawn between two nodes, indicating that two variables are independent after controlling for all other variables in the network. This is of particular interest since such a missing connection indicates one of the two variables could not have caused the other. As such, whenever there is a connection present, it highlights a potential causal pathway between two variables.

An increasingly popular method for controlling for spurious connections-as well as to obtain easier interpretable networks that may perform better in crossvalidation prediction-is to use statistical regularization techniques originating in the field of machine learning. The goal here is to obtain a network structure in which as few connections as possible are required to parsimoniously explain
the covariance among variables in the data. Especially prominent is to use of the 'least absolute shrinkage and selection operator' (LASSO; Tibshirani, 1996).
In essence, the LASSO shrinks partial correlation coefficients when estimating a network model, which means that small coefficients are estimated to be exactly zero. This results in fewer connections in the network, or in other words, a sparse network in which likely spurious connections are removed. The LASSO utilizes a tuning parameter $\lambda$ that needs to be set, controlling this level of sparsity. When the tuning parameter is low, only few connections are removed, likely resulting in too many spurious connections. When the tuning parameter is high, many connections are removed, likely resulting in too many true connections to be removed in addition to all spurious connections.

```{r, echo=FALSE, fig.align='center'}
exp_anal$plot.regularized_partial_correlation_network 
```


# Clustering

```{r, echo=FALSE}
# GO Slim annotation 
data(data_GOslim)
head(data_GOslim)

# GO Terms for enrichment
data(ORF2KEGG)
head(ORF2KEGG)
```



```{r, echo=FALSE, cache=TRUE, fig.show="hide", results="hide"}
gene_clust <- GeneClustering(data=pre_proc$data.wide, data_Symb=pre_proc$data.wide_Symb)
```


#### Clustering

We compute the manhattan distances between the knockouts' symbolised profile to cluster genes having relative distances equal to 0. 
We proceed to investigate the clusters which have at least 10 genes.

```{r, echo=FALSE, fig.align='center'}
gene_clust$stats.clusters
```

We then compute a profile plot for each cluster.

```{r, echo=FALSE,out.width = "140%", fig.align='center'}
gene_clust$plot.profiles 
```


#### Go Slim annotation


We now highlight the the biological process, the cellular component, and the molecular function of the genes within each cluster. 
We retain the annotations that map at least 5% of the genes in the cluster.
We do not include in the results the first cluster (Cluster 1) as this the "null" cluster which contains a mix of knockouts having no impact on the ions.
The first five entries of each cluster can be access as follows.

```{r, echo=FALSE}
lapply(gene_clust$stats.Kegg_Goslim_annotation, function(x) head(x,5))
```



#### Go terms enrichment

We perform the enrichment of the genes in the clusters by employing the all GO terms annotation in the [SGD online database](https://www.yeastgenome.org/).
The first five entries of each cluster can be access as follows.


```{r, echo=FALSE}
lapply(gene_clust$stats.Goterms_enrichment, function(x) head(x,5))
```



# Network analysis


```{r, echo=FALSE, cache=TRUE, fig.show="hide", results="hide"}
gene_net <- GeneNetwork(data=pre_proc$data.wide, data_Symb=pre_proc$data.wide_Symb)
```

The aim of the following network analysis is to group genes with same symbolic profile. 

First we compute the Manhattan distance between al gene pairs. This measure is then used in the linkage algorithm (method=single) and then the unique partition is found by cutting the hierarchical tree at zero-distances. 

For this analysis we filter the clusters as we do not consider the largest cluster as it contains genes with no phenotype as well as few smaller clusters of less than 10 genes.

We compute the empirical correlation matrix between genes (method = "pearson", use = "pairwise.complete.obs"), then we subset the correlation matrix based on the cluster filtering. Moreover, as we are interested only in positive correlations among genes we filter the correlation matrix based on an arbitrary correlation's thresold of 0.6 such that the clustering can be interpreted in terms of posivite correlation between gene profiles.

#### Network plot

We then generate the network from the described correlation matrix.
And finally we can visualise the corresponding network plot.


```{r, out.width = "140%",echo=FALSE}
gene_net$plot.pnet
```


#### Impact and betweeness scores

We are now interested to highlight the most central genes. To do so we can consider two metrics i.e. the impact and the betweeness. 
From the empirical correlation matrix between genes we can compute the betweenness measure as the fraction of shortest paths that pass through each gene (node). Next a measure of impact can be computed as the $L_2$ norm (Euclidean distance) of each gene.
These two centrality measures can be then used togheter to cluster the genes as follow.

```{r, out.width = "90%", echo=FALSE}
gene_net$plot.impact_betweenees
```


We can also access to the impact and betweeness value as follow (only first 10 value shown).

```{r, echo=FALSE}
head(gene_net$stats.impact_betweeness,10)
```

We can also associate each cluster to low or high values of impact and betwenees based on the highest number of genes in that cathegory. 

```{r, echo=FALSE}
gene_net$stats.impact_betweeness_by_cluster
```

